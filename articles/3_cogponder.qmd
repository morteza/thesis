
<!-- # Modeling Response Time in Cognitive Control Tasks using PonderNet Framework -->

# CogPonder: Towards a Computational Framework of General Cognitive Control

 
## Abstract {.unnumbered}

Current computational models of cognitive control are lacking in important ways. In psychology, cognitive control models tend to be designed for specific tasks which makes it hard to study cognitive control in general (e.g., across a battery of tasks, playing video games, or in real-life activities). Computer science, on the other hand, has been able to develop artificial agents capable of performing complex tasks but typically ignores resource limitations and how long it takes for an agent to make decisions and act. Response time is of the essence in human cognition and varies meaningfully depending on numerous factors, including in particular cognitive control which supports adapting behavior to environmental constraints to achieve specific goals. Recent work further points to the fact that cognitive control models could equally greatly benefit the development of a next generation of intelligent agents in computer science. Here we propose CogPonder, a flexible, differentiable end-to-end general cognitive control framework that is inspired by the Test-Operate-Test-Exit (TOTE) architecture [@miller1960] in psychology and by PonderNet [@banino2021] in computer science. CogPonder is a general deep learning framework that functionally decouples the act of control from the controlled decision making processes. The framework involves a controller that acts as a wrapper around any computational end-to-end model (that "perceive" the environment and generate "responses" on that environment) and controls when to stop processing and output a response (thus producing both a response and a response time). Here we implemented a simple instance of CogPonder and trained it to perform two classic cognitive control tasks (i.e., Stroop and N-back) while at the same time aligning its behavior to humans (i.e., similar responses and response times). The results show that across both tasks, CogPonder effectively learns from data to generate behavior that resembles the behavior of humans. This work thus demonstrates the value of this new computational framework of cognitive control and provides novel insights and research opportunities for both psychological and computer science.

 
## Introduction
The scientific study of human cognition has largely focused on how long it takes people to perform tasks (e.g., press a key in response to a light, multiply two numbers or name the capital of Luxembourg) and on what factors impact those response latencies (e.g., intensity of the light, magnitude of the numbers, familiarity of the content). There is a long and rich history of research on response times and many computational models have been developed to account for response time phenomena [@forstmann2016; @deboeck2019]. Furthermore, the study of response times is particularly relevant because in contrast to other measures, such as percent correct or IQ, response times express a physical quantity in a ratio scale [@jensen2006] which allows the direct comparison of raw measurements.

An important class of response times models derives from the drift diffusion model (DDM; @ratcliff1978) which is specifically designed to model binary decision making. It considers both the response (what choice the person made) and the response time (how long it took to make the choice) [see for instance, @ratcliff2008 and @ratcliff2016].  In this model, the stimulus triggers a stochastic (“noisy”) signal which is accumulated until it eventually reaches an upper or lower threshold (“decision bounds”)—the threshold that is reached determines the decision and the time when threshold is reached determines the response time. This type of model is appealing because it can account for a large range of behavioral data, has an intuitive computational interpretation (i.e., sequential probability ratio test) and seems to map well with neural decision-making signals [@gold2007; @forstmann2016]. Furthermore, models like the DDM can be fit to behavioral data and the underlying model parameters provide useful and meaningful quantities that help better understand human cognition (e.g., the quality of the signal, people's biases for one option versus another). Indeed, with this model it becomes possible to make principled predictions about the effect of task parameters (e.g., instructions emphasizing speed versus accuracy) on behavior (e.g., decrease in both response times and accuracy) via their impact on model parameters (e.g., decrease of the decision bound parameter). 

Of particular interest in this context are a family of tasks that relate to the psychological construct of cognitive control [@bagetta2016]. These tasks include for instance the Stroop task, Task-switching, the Go/No-go task, the Flanker tasks, and the N-back task, to name just a few. While cognitive control is a complex construct with a meaning that lacks consensus in the literature (see Chapter 1), one of its key properties is that it allows the cognitive system to regulate its processing to achieve particular outcomes (e.g., inhibit a prepotent response, maintain attentional focus), and this regulation of processes typically has a measurable impact on response times (i.e., control is effortful and takes time). Indeed, response times have long been the main variable of interest to cognitive control scientists, and computational models like the DDM have been used to capture these cognitive control effects on response times [see e.g., @ratcliff2018; @pedersen2022; @eisenberg2019].

Note that DDM is not a cognitive control model per se but rather a general two alternative decision making model. Adapting DDM to cognitive control settings would thus require additional machinery. Note also that there are computational models of cognitive control [e.g., @botvinick2014] that could be coupled with DDM. However, these cognitive control models are typically custom-made for specific tasks, meaning that the model for the Stroop task cannot be readily transposed to the N-back task for example.

The models mentioned above constitute major achievements in psychology and they provide invaluable insights into the human mind. They are, however, imperfect. For instance, DDM-like models apply to a limited class of tasks. They are adequate for speeded two alternative choice tasks but not for multi-alternative choice tasks [@REF] or tasks where the response is more complex than a choice (e.g., continuous tracking). Furthermore, these models do not in fact perform a task but instead generate data that looks like human data (i.e.,they are models of the data and not models of the cognitive processes). This is in contrast to “acting” models, like modern reinforcement learning models for instance, which may for instance receive the pixel values of images displayed on a computer screen as input and generate actions to play video games at human level performance (@mnih2015). Finally, models like the DDM are rather complex mathematical objects, without reliable closed-form solutions and are typically not differentiable. This makes it difficult to incorporate DDM in modern deep learning architectures that compute gradients to backpropagate errors and learn from data. These limitations are well-known and there are ongoing efforts to overcome them [e.g., @christie2019; @rafiei2022].

In recent years there have been tremendous advances in machine learning, with computational agents learning to perform highly complex tasks better than humans (e.g., modern video games, Go, Stratego). These models are interesting because they are "acting" models and they are generic (i.e., the same model architecture can be used to learn to perform many different tasks). They are, however, also limited in important ways. First, these large models typically lack structure that would facilitate the interpretation of the underlying computations. This is in contrast to computational cognitive control models that employ an adequate *level of computational abstraction* but then lack the ability to perform complex tasks. Secondly, by and large, the machine learning community hasn't yet picked up on the concept of *cognitive control* and the idea that machine learning models could regulate themselves to adapt their computations to the level of complexity of the task to be performed or the amount of available resources [@shenhav2017; @moskovitz2022]. A notable exception here is PonderNet [@banino2021] which we describe below. Finally, and related to the previous point, in contrast to researchers in psychology, researchers in machine learning have largely ignored response times, not only as a metric of interest (the time needed for a given, standard neural network to make a decision does not vary with the complexity of the task or the quality of the input; it depends only on the structure of the network), but also as a behavioral constraint for the artificial agent. There are many situations that require people to stop deliberating and commit to a decision. In RL models, it is common to place the agent in a sort of turn-based environment where its world stops, waiting for the agent to act [@ramstedt2019]. Artificial agents that could control how long they deliberate would be able to adapt to changing environmental constraints.

To summarize, computational models in psychology and in computer science have different strengths and weaknesses. There could be great benefits for both fields to cross fertilize ideas and develop new types of computational control models. The work presented here is an attempt to move in that direction.

## Desiderata for a general computational cognitive control framework
Our goal is to develop a computational framework for cognitive control models that would be valuable to both psychology and machine learning researchers and which combines the strength of their respective approaches. More specifically, we want our framework to have the following main features:

agency: the model is able to perform the task at hand; 
completeness: the model accounts for both responses and response times;
versatility: the same model can perform a wide range of tasks; this allows the study of performance across multiple tasks under a common computational framework;
modularity: the model allows to augment any end-to-end computational model with cognitive control abilities; this allows both for great flexibility in model architectures and interpretability.
learnability: the model is differentiable and can thus be integrated in state-of-the-art deep learning models and benefit from modern software (e.g., PyTorch, TensorFlow, or JAX) that use automatic differentiation for parameter optimization and GPUs for faster computing.
composition: the model forms a building block of sorts and multiple such building blocks may be arranged in structures (e.g., sequence, hierarchy) to perform complex tasks; this allows for scalability while controlling complexity.

The inspiration for our model comes from two primary sources; PonderNet from machine learning and TOTE from psychology. The following is a description of both before we describe our framework named CogPonder.

### PonderNet
PonderNet is a recently developed algorithm that adjusts the complexity of the computations executed by a neural network as a function of the complexity of the task and the input [@banino2021]. With PonderNet, the same neural network uses fewer computational steps to perform simple tasks than complex ones. The rationale behind PonderNet is straightforward. In addition to learning to perform a specific task (using a reconstruction loss function), the network evaluates at each time step whether to stop or continue computation. This halting behavior is determined by learning a halting probability distribution that is constrained by a hyperparameter (a temporal regularization term encouraging fewer computation steps and exploration). This approach is in stark contrast to traditional machine learning approaches where the complexity of the neural networks is determined by the size of the input, adjusted manually and set once and for all for a specific task.

PonderNet is interesting within the context of cognitive control. First, because PonderNet adjusts computational resources of a system based on the complexity of the task to be solved, it can be seen as a form of cognitive control. Second, by controlling the halting distribution, PonderNet highlights the conceptual importance of considering the time needed to perform a task (more exactly the number of computational steps). By doing so, PonderNet creates a bridge between the rich literature in experimental psychology grounded in the study of response times and the booming field of deep learning.

### TOTE
PonderNet is reminiscent of the famous cognitive control model named TOTE [@miller1960], where TOTE stands for Test-Operate-Test-Exit. In TOTE, as in PonderNet, computations (or operations) unfold in cycles with tests evaluating on each cycle if a specific condition is met and consequently deciding whether to exit (halt) the process or trigger a new cycle of operations. As in PonderNet, the control mechanisms are functionally separated from the operators. Interestingly, the main motivation behind the TOTE model was to address complex human behavior. While this might be achieved with PonderNet by increasing the complexity of the underlying operator, in TOTE, the authors argue that complex behaviors could be modeled by organizing multiple TOTE units in sequences, hierarchies or other structures. Under this view, TOTE units are computational building blocks that can be assembled to generate complex behaviors. With the advent of modern computers and computational tools it is now possible to translate the ideas behind TOTE in computational models capable of performing complex tasks.

## The CogPonder framework



Figure 1. to illustrate the idea of modularity/flexibility of the model
use same terms here as in the text (e.g., “halting network”) ; write text that corresponds to the figure (decision diamond is not mentionned anywhere yet).


The general idea behind the CogPonder framework is illustrated in Figure 1. The starting point for a CogPonder model instance is an end-to-end model, termed “Operator”, which on a given trial $n$ takes an input $X_n$ and outputs $y_n$ (see Figure 1a). This operator may for example be a deep neural network performing the Stroop task, in which case $X$ might be a textual description of the stimulus or the pixel values of the screen and $y$ might be a label of a color or a motor command to press a specific button. 

The key idea behind CogPonder is to disconnect the Operator from its direct inputs and outputs and to encapsulate the Operator inside a local virtual environment that is governed by the Controller. The Controller intercept both the inputs and outputs of the operator, and determines what inputs are fed to the Operator and ultimately what output to be emitted on a given trial. There are many possible ways to implement the Controller, and different types of control the Controller could exert on the Operator. Here we consider the Operator as a blackbox (i.e., the Controller has no read or write access to the Operators internal parameters) and use a formulation that is very similar to PonderNet (possible extensions are discussed in "Limitations and possible future extensions"). More specifically, within a given trial $n$ the Controller will repeatedly call the Operator, with each of these iterations being indexed by step $s$, until it decides to halt processing for trial $n$ and to emit a response $y_n$. The number of iterations performed on trial $n$, $s_n$, reflects the response time for that trial.

Following PonderNet, the decision to “halt” or to “continue” iterating at step $s$ is determined by a Bernouilli random variable $\Lambda_s$, with $\Lambda_s=1$ meaning “halt” and $\Lambda_s=0$ meaning “continue”. The conditional probability of halting at step $s$, given that the process was not halted in the previous step is given by:


$$
P(\Lambda_s = 1 | \Lambda_{s-1} = 0) = \lambda_s \qquad \forall \: 1 \leq s  \leq S
$$

where $S$ is the maximum number of steps allowed before halting.

From this expression one can compute the unconditioned probability of halting at step $s$:



$$
p_s = \lambda_s \prod_{j=1}^{s-1} (1-\lambda_j)
$$


Importantly, the value of $\lambda_s$ is computed by the Controller at each timestep $s$, endowing it with the power to adjust the system's computational complexity and determining its response time distribution $p_s$. The other major quantity that the Controller needs to compute on each iteration is $H_s$, the input to the Operator (using $X_n$ and $H_{s-1}$). Because $\lambda_s$ is computed from $H_s$ (see Figure 1), speed and accuracy are intrinsically coupled at the within-trial level. The Controller can be instantiated using neural networks and its parameters adjusted using standard methods and labeled data (see "Evaluation of a CogPonder model").

It is important to note that CogPonder is conceptually quite different from RTNet [@rafiei2022]. In RTNet, the same input is passed multiple times through a neural network with each pass using slightly different weights (i.e., weights are not fixed but sampled from a distribution) and the output of each pass is accumulated in a special output layer until reaching a decision threshold (similar to DDM). In CogPonder a given model is wrapped by a controller, the model is iteratively fed different inputs (they are generated by the Controller) and the response time (number of computational steps) is determined by computational requirements rather than resulting from stochasticity that is injected in the system. 

CogPonder is very similar but also different from PonderNet in the sense that CogPonder aims to align computational models with human behavior rather than adjusting computational resources of neural networks to the complexity of a particular task. CogPonder also aims to embrace the "building blocks" metaphor of TOTE and further our understanding of cognitive control (i.e., it aims to become a theoretical framework and not "only" a method).
 
## Evaluation of a CogPonder model
### Objectives and Rationale
This work aims to be a proof of concept, demonstrating the value of CogPonder to both psychology and computer science research. The preliminary work presented below has two main objectives:
demonstrate that the same CogPonder model instance can learn to perform two different cognitive control tasks from cognitive psychology; this is important because it shows tasks that have so far mostly been considered in isolation can now be investigated within a common computational framework.
demonstrate that the behavior of a CogPonder model aligned to human behavior is able to capture important patterns in the human data; this is important because it shows that CogPonder might be useful to understand behavior and might also be used to run simulation ("what if") experiments.  

### Dataset
Here we use a subset of the Self-Regulation Ontology dataset [publicly available and previously published in @eisenberg2019] which contains behavioral data from 521 of participants who completed computerized cognitive tests as well as questionnaires. In this study we consider only data from one human participant who completed two cognitive tasks: the Stroop test and the 2-back test. We chose these specific tasks because they have both been associated with the construct of cognitive control but are quite different in that they involve different types of stimuli (words versus letters), task instructions (name ink versus same/different), cognitive processes (involving the inhibition of a prepotent response versus updating memory) and responses options (2 versus 3 options).

*In the Stroop task*, participants were presented with a name of a color written in ink that was either congruent or incongruent with the word (e.g., the text "red" written in a blue color is incongruent, while the text "red" written in a red color is congruent) and they were instructed to report quickly and accurately the color of the ink (i.e., ignore the text) by pressing one of three keys (corresponding to the options red, green, blue). Each participant completed 24 practice trials and 96 test trials; here we consider only test trials.

*In the N-back task*, participants were presented with a stream of letters (e.g., "A", "X”, "a") and they were instructed to report for each letter whether it was the same letter as the one presented N letters ago (irrespective of capitalization) by pressing one of two keys corresponding to “same letter” (i.e., target) and “different letter” (i.e., non-target). Each participant completed several versions of the N-back task; here we consider only the cases where N=2 (i.e., 2-back trials), which amounted to 342 trials.

In both tasks, we use the trial-level data for participants which includes a description of the stimulus (e.g., "A"), trial index, the participants response (e.g., the choice of the response option "red") and time needed to make that response (i.e., response time, in milliseconds). For more details on the original datasets, see @eisenberg2019.

### Method
Our goal is to train the same computational cognitive control model (i.e., "agent") to perform both the Stroop and the 2-back tasks. In both cases, the model will receive as input a sequence of stimuli (i.e., color words or letters) and will generate a response to each stimulus (i.e., color words or same/different). Note that this is an "acting" model that is actually able to perform the task and not a "fitting" model that aims to fit patterns in the data. Note also that by responding to each stimulus, the data generated by the agent will have the same structure as the human data (i.e., trial-level data with a stimulus description, the choice made by the agent and the time it took the agent to make that decision).

In addition to training the agent to accurately perform the task, we want to *align* the agent with humans. By this we mean that we want to adjust the internal parameters of the computational model so that it will generate a behavior in response to stimuli that is similar to human behavior (e.g., similar response time distributions and accuracy levels).

This alignment is obtained by the following loss function, the value of which will be minimized during the training phase of the model (see "Model evaluation procedure"):


$$
L_{\text{total}}= L_{\text{response}} + \beta L_{\text{time}}
$$


This loss function comprises two terms which are weighted by the hyperparameter $\beta$. The first term aligns the agents choices with the choices made by human participants ("response reconstruction loss"):



$$
L_{\text{response}} = \sum_{s=1}^{S}{\mathcal{L}(\hat{y}_s}, y)p_s
$$

where $\mathcal{L}$ represents the cross entropy loss function.

The second term aligns the agent's response times (the distribution of the halting probability $p_s$) with the response times distribution of human participants $d$ using KL divergence ("time regularization term"):



$$
L_{\text{time}} = KL(p_s || d)
$$

It is important to note that computers typically perform tasks much faster than humans do and that depending on the specific computer hardware (or software), the time needed to respond may vary considerably. This means that elapsed computation time is not the relevant variable to track and that we should instead track the number of computational steps [@cormen2022]. In a given computational context (e.g., a particular task and performance constraint) this number may be stable despite the time needed to execute those steps varying significantly depending on the underlying hardware. 

Equation 3 ($L_{\text{time}}$) requires computing the similarly (via KL divergence) between the distribution of halting times, which are expressed in number of steps, and participants response times distributions, which are expressed in milliseconds in our dataset. To compute this term it is necessary to either convert number of steps into milliseconds (e.g., using a hyperparameter that expresses the duration per step) or to convert the response times from milliseconds to number of steps (e.g., using a hyperparameter that expresses duration per step and dividing the response time by that duration). We used the second approach and manually determined an adequate value for the step duration hyperparameter (see "Model evaluation procedure").

### Model evaluation procedure

#### Model architecture: CogPonder instantiation
Figure 1b describes the general template for a CogPonder model. CogPonder is a framework that can be instantiated in many different ways. Here we chose a specific implementation to perform the Stroop and N-back tasks, noting nevertheless that other implementations are equally valid and that for other tasks more complex instantiations might be needed. Our goal is to demonstrate the value of the framework, not the value of this specific instantiation of the framework.

For the Operator in the model (see Figure 1b) we used a simple neural network with one dense linear layer and ReLU activation. The Controller includes two separate networks: a recurrent network and a halting network. The recurrent network is a GRUCell that iteratively computes inputs to the Operator. At each iteration $s$ it computes $H_s$ and serves it as the input to the Operator. The halting network approximates the probability of halting at each time step ($lambda_s$). It is a fully connected linear layer with ReLU activation that receives as input $H_s$ and determines the halting of the CogPonder model at a given time point $s$ within a trial and the emission of the output for that trial.

Finally, the decision to halt or to continue processing is made at each processing step $s$ within a given trial based on a biased coin flip (Bernoulli sample with probability of $\lambda_s$), which is emitted by the halting network (see Figure 1b).

Note that the same model architecture was used to fit the Stroop and N-back tasks (separately) but there were slight differences between these two cases because the stimuli and responses are different in the two tasks. More specifically, a stimulus in the Stroop task is encoded using 2 inputs (color and word), while a stimulus in the N-back task requires 6 inputs (one-hot encoded letters). Similarly, in the Stroop task, the network needs to emit one of 3 choices while in the N-back only one of two choices. This being said, it is straightforward to extend these models so the exact same model architecture could apply to both cases.

#### Model training 
Here we present preliminary work to align CogPonder to human data. CogPonder was fit to a single participant taken at random from the dataset and separately for the Stroop and the N-back tasks (i.e., different sets of parameters were adjusted for each task). Participants' data in each task represents a time series (i.e., trials are ordered and there is a dependency across trials). This data was split into 75% training set and 25% test set, corresponding to 72 train and 24 test trials in the Stroop task and 256 train and 86 test trials in the N-back task.

The training involved a maximum of 10000 epochs (i.e., loops over the dataset) which was stopped when no improvement was observed in minimizing total validation loss (early stopping with 0.01 patience on the validation $L_{\text{total}}$). We used stochastic gradient descent (Adam optimizer) to minimize $L_{\text{total}}$ (see loss function in Equation 3). All model parameters within the Operator and Controller were adjusted simultaneously and using the same procedure with the exception of the step duration hyperparameter which for this preliminary analysis was set manually to 20ms. In total, 62 parameters were adjusted for the Stroop task and 239 parameters for the N-back task and it takes around 15 minutes to fit one participant on one task on an average laptop.

The evaluation of the model used the 25% of trials that were not used for training. Once the model parameters are set, the model can be used to generate behavior (i.e., responses and response times) in response to stimulus sequences. This artificial agent generated behavior can then be compared with human generated behavior using standard descriptive statistics such as average accuracy and average response time for example.

#### Step duration hyperparameter
As a first approximation we manually tested several values (10ms, 20ms, 50ms, 100ms) and selected the value of 20ms as this seemed to lead to the best alignment with human data and faster convergence of the model parameters. In a future iteration of this analysis, this hyperparameter will be estimated directly from the data using a dedicated validation set. 

#### non-decision time hyperparameter
We also assumed that the minimum human response time for each task was the non-decision time. We subtracted this minimum response time from the other response times, resulting in time steps that range from 1 to $max(RT) - min(RT) + 1$. Compared to the raw response times, this choice resulted in faster convergence of the model parameters. In a future iteration of the analysis, non-decision time will be treated as a hyperparameter and estimated from the data.

## Results
 
Our first goal is to determine if the same CogPonder model can learn to perform two different tasks using data from one human participant. Figure 2 shows the total loss ($L_{\text{total}}$, as defined in Equation 3) computed on the test data as a function of the number of epochs during the training phase. It is apparent from this figure that CogPonder does indeed learn in both tasks, with the loss reaching an asymptote after about 400 epochs (i.e., iteration through the training dataset). This figure also demonstrates that because of its design, CogPonder (like PonderNet) can take advantage of modern deep learning software to efficiently fit complex models.

 

Figure 2. Cogponder learns to behave like humans. With increasing learning iteration (epochs) the loss decreases and asymptotes. This is true both when aligning CogPonder with the Stroop task (red curve) or with the N-back task (blue curve). Note that the two tasks were trained and tested separately.


Our second goal is to determine to what extent a CogPonder model *acts* like a human once it has been trained with human data. Because CogPonder is an acting model it generates trial-by-trial responses that have the same data shape and type as human responses. This allows to directly compare the behavior of CogPonder and human agents using the same descriptive statistics and data visualization code. As a first step, we compare the average accuracy and average response time of a human versus CogPonder agent in both the Stroop and N-back tasks (see Figure 3). It is apparent from Figure 3 that CogPonder is able to capture these broad patterns in the human data. In particular, for both the Stroop and N-back tasks, CogPonder produces a behavior with accuracy levels and response speeds that are in the same ballpark as human data when considering all types of trials ("All" label in the x-axis of Figure 3).

Next, we investigated to what extent CogPonder was able to reproduce finer grained human phenomena. To do so, we plotted average accuracy and average response time as a function of conditions (see Figure 3), as well as the distribution of response times for both the human and the CogPonder agent, separately for the Stroop and N-back tasks (see Figure 4). The "fits" are obviously not perfect. For example, while the human data shows a congruency effect in the Stroop task, whereby accuracy is lower and response times longer in incongruent trials than in congruent trials, no such effects are apparent in the CogPonder data. One should note however that the error bars are quite large and that it remains plausible that with a larger training dataset, CogPonder will be able to capture these Stroop effects. What is most encouraging is these results is the similarity between the response time distributions of the human participant and the CogPonder agent in both the Stroop and N-back tasks. Overall, these results suggest that CogPonder is able to mimic important markers of human behavior, which makes CogPonder a promising new approach to the study of human cognition.








Figure 3. CogPonder behavior is comparable to human behavior. CogPonder captures the overall pattern of average accuracy (left column of panels) and average response times (right column of panels) in both the Stroop task (upper row of panels) and in the N-back task (bottom row of panels) when grouping all types of trials ("All"). However, when separating trials by type ("congruent" and "incongruent" in the the Stroop task and "target" and "non-target" in the N-back task), some discrepancies are observed. Error bars show 95% confidence intervals.






Figure 4. CogPonder also mimics finer grained phenomena (e.g., response time distributions).


## Discussion
The present work is a first step towards developing CogPonder, a computational cognitive control framework that can be applied to a broad range of use-cases—including in particular batteries of cognitive tests. In this framework, cognitive control is envisioned as a model that wraps around any end-to-end operator model and controls both its inputs and outputs to achieve a desired performance profile. In this work we focused in particular on two classic experimental psychological tests (the Stroop and the N-back tests) and showed that a basic instance of CogPonder can be trained to align with human behavior and will then generate behavior that captures some key patterns in the human data, in particular average accuracy and response time as well as response time distributions. While these results are still preliminary and more work is needed to fully explore the capabilities of CogPonder, this work constitutes a proof of concepts and speaks for the value of the CogPonder framework. 

CogPonder is unique in that it satisfies a number of important desiderata that are only partially satisfied by current models in cognitive sciences. First of all, CogPonder has *agency*—meaning it is an architecture that is able to perform tasks (e.g., make timed decisions when faced with particular stimuli). This is in contrast to models that focus on describing the structure of the data.   

Second, CogPonder is *complete* in the sense that its behavior can have all the same dimensions as human behavior. This is in contrast to models that account only for the choices made by an agent but not their response times.

Third, and most importantly, CogPonder is *versatile* in the sense that it can in principle perform a wide range of tasks. In the present study, we focused only on two tasks but there is no reason this framework cannot account for a much broader range of tasks. This is in contrast to models that are tailored for individual tasks and limit our ability to use the model to understand cognitive control in general (i.e., across many tasks). 

Fourth, the model is *modular* in the sense that different control models may be used to wrap any type of end-to-end model. This feature is important because it allows the development of models that are both flexible (i.e., can adapt to a large range of use cases), while at the same time offering interpretability (i.e., it's clear which effects can be attributed to the controller versus the operator). Fifth, CogPonder is *learnable* in the sense that the controller model is differentiable and can thus be incorporated into modern deep learning software that is highly effective to train large models on big datasets. This feature of CogPonder facilitates the use of CogPonder in practice, compared for example to models that require custom made code and fitting procedure. Finally, we believe, but haven't yet shown, that CogPonder allows for model *composition*. By this term we mean that CogPonder can be seen as a building block that models a local aspect of cognitive control and multiple CogPonder units may be chained or organized into hierarchical structures in order to achieve highly complex behavior while limiting the complexity of the overall computational model.

## Implications
The present study shows that CogPonder can be applied to multiple tasks and is able to account for both responses and response times. 

The implication for psychology is that CogPonder now offers new opportunities to study behavior and in particular cognitive control across a large range of tasks (e.g, beyond the Stroop test, beyond the two alternative choice family of cognitive tasks) using a common framework. This is important as it provides a common theoretical and computational ground to investigate human behavior. There are, in particular, two use-cases where we believe CogPonder will be particularly useful. The first use-case relates to simulations and the ability for CogPonder models to run counterfactual "what if" experiments. More specifically, if we have computational models that can account for multiple cognitive tasks, one could use these cognitive models to develop new cognitive tasks that may be more diagnostic of certain model parameters or may help discriminate between competing computational models. The second use-case relates to cognitive training and transfer. There is currently a lack of quantitative theories that would allow one to predict how one person would perform a new task (given some historical data about that person), nor how exactly cognitive training would transfer to which other tasks and how much exactly performance should improve on those tasks. Multitask computational models of cognition are necessary to understand transfer and CogPonder is one way to develop such models.

Finally, it is also important to note that current models in computational psychology focus on modeling tasks that are relatively simple (e.g., the Stroop test) and are inadequate to model more complex human behavior (e.g., video game play). It is not obvious how models developed for the simpler tasks could be extended to grasp more of the complexity of human behavior. This is not the case for CogPonder. Because of its properties, it is rather conceptually straightforward to expand CogPonder to develop agents able to perform any task modern AI is able to solve. Thus an important achievement of CogPonder is its ability to break a complexity of behavior ceiling relative to existing approaches. 

The present work also has numerous implications in computer science. As explained earlier, most current models in AI (i.e., deep learning, RL) have not yet caught up on the importance of response times and cognitive control as valuable modeling concepts. Currently, the focus in these fields is mostly on developing models that are able to perform difficult tasks with the highest possible level of accuracy, irrespective of the computational resources (for training and computation) and training data needed to achieve those accuracy levels. This strategy is clearly valuable and is quickly pushing the boundaries of AI. However, there is obviously the need to also develop computational models that can adjust their internal complexity to the complexity of a task to be solved (cf. PonderNet) and to the fluctuating demands of the environment. An artificial agent, acting and learning in the world, may not have the luxury of quasi infinite resources and unlimited time to act and may instead have to commit to quick, albeit less accurate decisions, the same way humans do. The CogPonder framework provides a principled way to extend modern end-to-end models developed with a focus on maximizing accuracy in a way that allows for graded, time-sensitive, and adaptive computation. Finally, AI aims to develop agents that are able to perform highly complex tasks (e.g., making pizza). A major challenge in this context is to control complexity so that models can be effectively trained using a reasonable amount of data. We believe that CogPonder, and in particular its potential for composition, may provide an interesting solution to this problem.


## Limitations and future extensions

The current work is a proof of concept, and as such, it has obvious limitations that future work will address. First, there are improvements that can be made to implementation of the CogPonder model and its evaluation. For example, in the above work we set some hyperparameters manually instead of learning their values from data. Second, we trained the model on only one participant's data. In future iterations we will align the model to a larger set of participants and evaluate to what extent the CogPonder can capture inter-individual differences. Applying CogPonder to groups of participants may also require rethinking the CogPonder training procedure to allow for hierarchical as well as shared model parameters across participants. Third, we only tested two cognitive tasks, the Stroop and the N-back task, and only performed limited descriptive analysis to compare human and agent data on those tasks. In future work, we will more systematically explore CogPonder's ability to perform cognitive tests and develop finer grained analyses to assess its behavior. In particular, we aim to integrate CogPonder in the CogEnv virtual cognitive task environment (see Chapter 2) and develop automated data analysis pipelines that apply to both human and artificial data. Finally, although we showed that the same CogPonder model can be trained to perform different tasks, we have not yet investigated the relationships between those two trained model instance (e.g., are model parameters similar across the two tasks) nor have we trained a model to jointly perform both tasks (e.g., by including a task description as an input to the system). These steps seem crucial to assess the value of CogPonder as a theoretical model for cognitive control in psychology.

Although CogPonder is already a very flexible framework, there are several ways in which it could be further be extended, both inwards (i.e., changing the mechanics of CogPonder) and outwards (i.e., changing how CogPonder interfaces with other modules). In the current work, the Controller controls only one Operator. In more advanced versions, CogPonder could encapsulate and orchestrate multiple, perhaps competing Operators in parallel. Furthermore, in the current work, the Operator is conceived as a black box—a module that could be imported as is, without having to expose its internal workings and parameters. This is an interesting property from an engineering point of view as it clearly separates the development and testing of Operator models from the development and testing of Controller models. If, however, the Controller has reading and writing rights to the internals of the Operator, the Controller could be endowed with much greater control abilities (e.g., set or reset model weights, learn to continuously predict accuracy of the Operator based on the values of its internal parameters).  Also, in the current work, the Controller focuses only on the current trial and on learning what inputs to provide to the Operator to achieve a desired outcome. But there are other roles that the Controller could play. For instance, the Controller could have a much more active role in the training of the Operator. This could be achieved for example by controlling the learning rate of the Operator but also by controlling what data to use for learning. For example, CogPonder could maintain an internal dataset—using historic ("episodic memory") or synthetic data (e.g., generated from a time-consuming process that the system aims to automate)—and train the Operator to perform well on that dataset. This type of mechanism would allow for offline ("replay") learning, and could be useful to achieve overall better performance with fewer new observations.

In addition to extensions that could be envisioned for the inner workings of CogPonder, there are also extensions in line with the "building-blocks" view of the TOTE model that might be worth investigating further. In the current implementation, CogPonder receives as input the stimulus description and outputs the response. It would make sense however to consider CogPonder as a piece of a larger system rather than the system as whole. Even in the case of simple response times, computational models in psychology have argued for the need to model not only the decision process but also other processes involved in the task (including for example, the transduction of photons to action potentials in the retina, the transmission of signals from the retina to the visual cortex, and the transmission of action potentials from the motor cortex to skeletal muscles)—these processes are typically lumped together and modeled as a non-decision process whose duration is added to the decision time to form the response time. In addition to providing more detailed accounts of simple tasks, composing CogPonder networks into more complex neural architectures may provide a means to model planning and performance in complex sequential tasks. This is a key idea of TOTE: by organizing relatively simple TOTE building blocks into hierarchies and sequences it becomes possible to orchestrate and control complex sequences of behavior, such as preparing a pizza for example. CogPonder provides a principled way to build and train those building blocks; but much work is still needed to evaluate what exactly can be construed with them—we hope this preliminary work on CogPonder ignites interest in these exciting new lines of research, both in psychology and computer science.

